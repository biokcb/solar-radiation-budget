{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Anomalies Calculation Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get things\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from netCDF4 import Dataset \n",
    "import numpy.ma as ma \n",
    "\n",
    "filename1 = \"HadISST_ice.nc\"\n",
    "ds1 = Dataset(filename1, mode=\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF3_CLASSIC data model, file format UNDEFINED):\n",
      "    Title: Monthly version of HadISST sea ice component\n",
      "    description: HadISST sea ice concentration.\n",
      "    institution: Met Office Hadley Centre\n",
      "    source: HadISST\n",
      "    reference: Rayner, N. A., Parker, D. E., Horton, E. B., Folland, C. K., Alexander, L. V., Rowell, D. P., Kent, E. C., Kaplan, A.  Global analyses of sea surface temperature, sea ice, and night marine air temperature since the late nineteenth century J. Geophys. Res.Vol. 108, No. D14, 4407 10.1029/2002JD002670\n",
      "    Conventions: CF-1.0\n",
      "    history: 8/4/2016 converted to netcdf from pp format\n",
      "    supplementary_information: Updates and supplementary information will be available from http://www.metoffice.gov.uk/hadobs/hadisst\n",
      "    comment: Data restrictions: for academic research use only. Data are Crown copyright see (http://www.opsi.gov.uk/advice/crown-copyright/copyright-guidance/index.htm)\n",
      "    dimensions(sizes): time(1754), latitude(180), longitude(360), nv(2)\n",
      "    variables(dimensions): float32 \u001b[4mtime\u001b[0m(time), float32 \u001b[4mtime_bnds\u001b[0m(time,nv), float32 \u001b[4mlatitude\u001b[0m(latitude), float32 \u001b[4mlongitude\u001b[0m(longitude), float32 \u001b[4msic\u001b[0m(time,latitude,longitude)\n",
      "    groups: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds1) # checking the data set variables, dimensions, sizes, etc so I can use that info later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "#pull out relevant data\n",
    "time = ds1.variables['time'][0:180]\n",
    "lat = ds1.variables['latitude'][:]\n",
    "lon = ds1.variables['longitude'][:]\n",
    "toa = ds1.variables['sic'][0:180,:,:]\n",
    "skin = ds1.variables['time_bnds'][0:180,:]\n",
    "\n",
    "# this part I was just checking the attributes for each variable so I could save it all the same again later. \n",
    "print(np.shape(time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5479.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(skin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(skin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(toa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(toa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating anomalies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating monthly averages & monthly deviations\n",
    "\n",
    "Here we are taking into account the natural variation between months. The cell below just gathers all the indices corresponding to each month over all available (complete) years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  12  24  36  48  60  72  84  96 108 120 132 144 156 168]\n",
      " [  1  13  25  37  49  61  73  85  97 109 121 133 145 157 169]\n",
      " [  2  14  26  38  50  62  74  86  98 110 122 134 146 158 170]\n",
      " [  3  15  27  39  51  63  75  87  99 111 123 135 147 159 171]\n",
      " [  4  16  28  40  52  64  76  88 100 112 124 136 148 160 172]\n",
      " [  5  17  29  41  53  65  77  89 101 113 125 137 149 161 173]\n",
      " [  6  18  30  42  54  66  78  90 102 114 126 138 150 162 174]\n",
      " [  7  19  31  43  55  67  79  91 103 115 127 139 151 163 175]\n",
      " [  8  20  32  44  56  68  80  92 104 116 128 140 152 164 176]\n",
      " [  9  21  33  45  57  69  81  93 105 117 129 141 153 165 177]\n",
      " [ 10  22  34  46  58  70  82  94 106 118 130 142 154 166 178]\n",
      " [ 11  23  35  47  59  71  83  95 107 119 131 143 155 167 179]]\n"
     ]
    }
   ],
   "source": [
    "#pull out which indices would correspond to the same month in our dataset\n",
    "months=np.array([0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "all_months=np.array([0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "\n",
    "for i in range(1,15): #each month occurs 15 times except for two months extra\n",
    "    months+=12\n",
    "    all_months=np.vstack((all_months, months))\n",
    "\n",
    "all_months = np.transpose(all_months) #so that it is divided properly\n",
    "\n",
    "print(all_months) #check that it works, just missing the last two months which I add back in later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize variables in same shape as toa/skin arrays. \n",
    "toa_loc_monthly = np.empty([180,180,360])\n",
    "skin_loc_monthly = np.empty([180,2])\n",
    "\n",
    "\n",
    "#calculate monthly averages and deviations from those averages\n",
    "for i in range(1,13):\n",
    "    submonths = all_months[i-1] #all others were just 15 years exactly\n",
    "    \n",
    "    #actual calculation for each month, stored in the same places as original data\n",
    "    toa_loc_monthly[submonths,:,:] = toa[submonths,:,:] - np.mean(toa[submonths,:,:],0)\n",
    "    skin_loc_monthly[submonths,:] = skin[submonths,:] - np.mean(skin[submonths,:],0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180L, 180L, 360L)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(toa_loc_monthly_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual Deviations from Average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the average over time at each lat/lon position\n",
    "toaAvgLL = np.mean(toa[:,:,:],0)\n",
    "skinAvgLL = np.mean(skin[:,:],0)\n",
    "all_years = np.transpose(all_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_av = np.empty([180,2])\n",
    "toa_av = np.empty([180,180,360])\n",
    "\n",
    "for i in range(1,16):\n",
    "    skin_av[all_years[i-1],:] = np.mean(skin[all_years[i-1],:],0)-skinAvgLL\n",
    "    toa_av[all_years[i-1],:,:] = np.mean(toa[all_years[i-1],:,:],0)-toaAvgLL\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2556.822265625"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(skin_av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2557.01123046875"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(skin_av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the new data as a netCDF\n",
    "\n",
    "This section takes our newly calculated variables and saves them into a netCDF format. If you calculate more variables, just add them using the format of a variable it is similar too. Just be sure to use a unique name. This section will not be necessary when we convert it to a visit script since we will just directly plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new data as new netCDF file\n",
    "\n",
    "#sources\n",
    "#http://salishsea-meopar-tools.readthedocs.org/en/latest/netcdf4/\n",
    "#http://nbviewer.jupyter.org/urls/bitbucket.org/salishsea/tools/raw/tip/I_ForcingFiles/Initial/PrepareTS.ipynb\n",
    "#http://stackoverflow.com/questions/28462521/saving-climatology-result-to-netcdf-file\n",
    "\n",
    "\n",
    "anom = Dataset('new_icepack.nc','w')\n",
    "anom.createDimension('time', 180)\n",
    "anom.createDimension('lat', 180)\n",
    "anom.createDimension('lon', 360)\n",
    "anom.createDimension('nv', 2)\n",
    "\n",
    "anom_lat = anom.createVariable('lat', 'float32', ('lat'), zlib=True)\n",
    "anom_lat.long_name='Latitude'\n",
    "anom_lat.units = 'degrees_north'\n",
    "anom_lat[:]= lat\n",
    "\n",
    "anom_lon = anom.createVariable('lon', 'float32', ('lon'), zlib=True)\n",
    "anom_lon.long_name='Longitude'\n",
    "anom_lon.units = 'degrees_east'\n",
    "anom_lon[:]= lon\n",
    "\n",
    "anom_time = anom.createVariable('time', 'int32', ('time'), zlib=True)\n",
    "anom_time.units = 'days since 2000-03-01 00:00:00'\n",
    "anom_time.longname = 'time'\n",
    "anom_time[:] = time\n",
    "\n",
    "anom_toag = anom.createVariable('toa_av', 'float32', ('time','lat','lon'), zlib=True, fill_value=-999.0)\n",
    "anom_toag.units = 'Ice m-2'\n",
    "anom_toag.longname = 'Annual Ice Pack'\n",
    "anom_toag.coordinates = 'time lat lon'\n",
    "anom_toag.valid_range= (-400,400)\n",
    "anom_toag[:] = toa_av\n",
    "\n",
    "anom_skg = anom.createVariable('skin_av', 'float32', ('time','nv'), zlib=True, fill_value=-999.0)\n",
    "anom_skg.units = 'K'\n",
    "anom_skg.longname = 'Time_bnds'\n",
    "anom_skg.coordinates = 'time nv'\n",
    "anom_skg.valid_range= (-400,400)\n",
    "anom_skg[:] = skin_av\n",
    "\n",
    "anom.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
